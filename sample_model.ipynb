{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbc3fad-ba9f-4a0b-804e-a88a0761468b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !gdown --folder https://drive.google.com/drive/folders/18fbeQOzN4BMn09LPnFgWflhTP-r9JJrc?usp=sharing\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f0d612-020d-4599-8c9f-629fc20874be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from engine.data import get_data_loader\n",
    "from engine.metrics import accuracy\n",
    "from engine.model_base import LightningBaseModule\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3f86db-e733-478f-b35f-365e7137fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_data_loader(\"train\")\n",
    "val_loader = get_data_loader(\"val\")\n",
    "test_loader = get_data_loader(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3556eb3b-8ad4-4875-b033-2f17bf0a4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 80\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(EMBEDDING_SIZE, 10)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        output, _ = self.attention(X, X, X)\n",
    "        return self.activation(output)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attentions = nn.Sequential(*[Attention() for _ in range(3)])\n",
    "        self.layer_norm = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.attentions(X)\n",
    "        output = self.layer_norm(output)\n",
    "        return output + X\n",
    "\n",
    "\n",
    "class AttentionModel(LightningBaseModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention_1 = nn.Sequential(*[AttentionBlock() for _ in range(5)])\n",
    "        self.attention_2 = nn.Sequential(*[AttentionBlock() for _ in range(5)])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(8000, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 22),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.attention_1(X)\n",
    "        output = self.attention_2(output + X)\n",
    "        output = self.flatten(output)\n",
    "        output = self.dense(output)\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return [optimizer], [\n",
    "            {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_accuracy\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812de5a6-41fa-47c8-ac9c-118191f16a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | attention_1 | Sequential       | 389 K \n",
      "1 | attention_2 | Sequential       | 389 K \n",
      "2 | flatten     | Flatten          | 0     \n",
      "3 | dense       | Sequential       | 8.0 M \n",
      "4 | loss        | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.209    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/miniconda3/envs/dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 157/837 [00:07<00:33, 20.37it/s, v_num=25, train_step_loss=2.620, val_step_loss=2.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/miniconda3/envs/dl/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        patience=10,\n",
    "        min_delta=1e-4,\n",
    "    )\n",
    "]\n",
    "\n",
    "model = AttentionModel().cuda()\n",
    "trainer = pl.Trainer(max_epochs=5, callbacks=callbacks)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
